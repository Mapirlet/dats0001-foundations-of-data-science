{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05080d5f-640d-4a95-bba4-f0dadda33ce5",
   "metadata": {},
   "source": [
    "# `nb03c`: Visualizing high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c3f41f-e7b5-4fa3-a499-8f974656e110",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Visualizing high-dimensional data is a challenge, as it is difficult to visualize more than 2 dimensions. One way to visualize high-dimensional data is to use dimensionality reduction techniques to reduce the data to 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f476e35-a0f2-4b45-98e4-128cd370a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07754761-4c51-4d7b-bb4a-492a86653140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits(n_class=6)\n",
    "X, y = digits.data, digits.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7ae76-5403-4dc1-b10d-9ad7202bbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
    "\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49776336-46b3-4954-9ab5-5575f0a36859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "def plot_embedding(X):\n",
    "    _, ax = plt.subplots()\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "    for digit in digits.target_names:\n",
    "        ax.scatter(\n",
    "            *X[y == digit].T,\n",
    "            marker=f\"${digit}$\",\n",
    "            s=60,\n",
    "            color=plt.cm.Dark2(digit),\n",
    "            alpha=0.425,\n",
    "            zorder=2,\n",
    "        )\n",
    "    shown_images = np.array([[1.0, 1.0]])  # just something big\n",
    "    for i in range(X.shape[0]):\n",
    "        # plot every digit on the embedding\n",
    "        # show an annotation box for a group of digits\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 4e-3:\n",
    "            # don't show points that are too close\n",
    "            continue\n",
    "        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n",
    "        imagebox = offsetbox.AnnotationBbox(\n",
    "            offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i]\n",
    "        )\n",
    "        imagebox.set(zorder=1)\n",
    "        ax.add_artist(imagebox)\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f525a-ab2a-445e-8b66-6ba77ee9730c",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Principal component analysis (PCA) is a dimensionality reduction technique that finds the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890707f4-e124-4a5e-abd0-14a2e21debc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "Xt = pca.fit_transform(X)\n",
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad65e91-b032-4e53-b3c4-67e285650a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff85a2",
   "metadata": {},
   "source": [
    "Mathematically, consider a dataset $X \\in \\mathbb{R}^{n \\times d}$. The goal of PCA is to identify a orthogonal linear transformation $U \\in \\mathbb{R}^{d \\times k}$ that minimizes the reconstruction error of the data when projected onto a $k$-dimensional subspace. That is, $$\\min_{U} \\sum_{i=1}^n ||X_i - UU^TX_i||^2$$ subject to $U^TU = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91593bee-5365-4b3e-b77d-4f54001d4cef",
   "metadata": {},
   "source": [
    "## MDS\n",
    "\n",
    "Multi-dimensional scaling (MDS) is a dimensionality reduction technique that finds a 2D representation of high-dimensional data that preserves the distances between the data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21187783-d358-4b81-8642-ccc5f894fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2)\n",
    "Xt = mds.fit_transform(X)\n",
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec67f9-8c59-4dfd-b3df-ea21fc0d9810",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "T-distributed stochastic neighbor embedding (t-SNE) is a dimensionality reduction technique that finds a 2D representation of high-dimensional data that preserves the local structure of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119cc24-8edb-416b-a7c2-a4ee29b4305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\")\n",
    "Xt = tsne.fit_transform(X)\n",
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d052a1-4521-44c4-9797-c4c5146afec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817bed20-0f4f-4863-b0fc-7c7d6642a922",
   "metadata": {},
   "source": [
    "See also [Wattenberg et al, 2016](https://distill.pub/2016/misread-tsne/) on how to use t-SNE effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dats0001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
