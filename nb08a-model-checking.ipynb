{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05080d5f-640d-4a95-bba4-f0dadda33ce5",
   "metadata": {},
   "source": [
    "# `nb08a`: Model checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72dc22d-a680-4c48-aebe-20cd35508c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ba160",
   "metadata": {},
   "source": [
    "Now that we can simulate data and fit models, we can evaluate the quality of a model and critically assess whether it is a good fit to data. This is called model checking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fc582-be0b-4a77-b2d4-de807ddcd248",
   "metadata": {},
   "source": [
    "# The light speed experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9e0e2-9441-4dd8-aa70-8c2186a0c15f",
   "metadata": {},
   "source": [
    "In 1882, Simon Newcomb conducted an experiment to measure the speed of light. He used a rotating mirror to reflect a beam of light to a distant mirror and back. The time taken for the light to travel to the distant mirror and back was measured by timing the rotation of the mirror. The experiment was repeated several times, and the times taken for the light to travel to the distant mirror and back were recorded as deviations from 24,800 nanoseconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008799a-2043-44a7-bba0-2a41bdc43a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "light_speed = np.array([28, 26, 33, 24, 34, -44, 27, 16, 40, -2, 29, 22, 24, 21, 25,\n",
    "                        30, 23, 29, 31, 19, 24, 20, 36, 32, 36, 28, 25, 21, 28, 29,\n",
    "                        37, 25, 28, 26, 30, 32, 36, 26, 30, 22, 36, 23, 27, 27, 28,\n",
    "                        27, 31, 27, 26, 33, 26, 32, 32, 24, 39, 28, 24, 25, 32, 25,\n",
    "                        29, 27, 28, 29, 16, 23]) \n",
    "# light_speed = np.array([28, 26, 33, 24, 34, 27, 16, 40, -2, 29, 22, 24, 21, 25,\n",
    "#                         30, 23, 29, 31, 19, 24, 20, 36, 32, 36, 28, 25, 21, 28, 29,\n",
    "#                         37, 25, 28, 26, 30, 32, 36, 26, 30, 22, 36, 23, 27, 27, 28,\n",
    "#                         27, 31, 27, 26, 33, 26, 32, 32, 24, 39, 28, 24, 25, 32, 25,\n",
    "#                         29, 27, 28, 29, 16, 23]) \n",
    "\n",
    "len(light_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(light_speed, bins=20, edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedebddc-f2e5-45c8-a841-47eabe0dd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from scipy.stats import norm, uniform\n",
    "\n",
    "def log_likelihood(theta, x):\n",
    "    return norm.logpdf(x, loc=theta[0], scale=theta[1]).sum()\n",
    "\n",
    "def log_prior(theta):   \n",
    "    return (uniform.logpdf(theta[0], loc=-1000, scale=2000) +  # p(mu) = U[-1000, 1000]\n",
    "            uniform.logpdf(theta[1], loc=0.1, scale=1000))     # p(sigma) = U[0.1, 1000]\n",
    "\n",
    "def log_posterior(theta, x):\n",
    "    return log_likelihood(theta, x) + log_prior(theta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6a097-8ea9-415e-a03c-19f729205ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior inference\n",
    "import emcee\n",
    "pos = np.array([10.0, 10.0]) + 0.1 * np.random.randn(10, 2)\n",
    "nwalkers, ndim = pos.shape\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(light_speed,))\n",
    "sampler.run_mcmc(pos, 300000 // nwalkers, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da222707-5457-4291-a95c-462fb09f5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ndim, figsize=(10, 7), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "labels = [\"mu\", \"sigma\"]\n",
    "\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:5000, :, i], \"r\", alpha=0.1)\n",
    "    ax.plot(np.mean(samples[:5000, :, i], axis=1), \"r\", alpha=1.0)\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d833096-59e3-4468-964d-90966732c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = sampler.get_autocorr_time()\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e72db1-f18f-4c06-959d-4e7fcbf607d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "thetas = sampler.get_chain(flat=True, discard=500, thin=40)\n",
    "fig = corner.corner(thetas, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53452b6-50e9-4f71-95ae-7462dd353683",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"E_p(mu|x)[mu] =\", np.mean(thetas[:, 0]))\n",
    "print(\"E_p(sigma|x)[sigma] =\", np.mean(thetas[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b994d89-c8e2-476f-86df-e35bf4337bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Empirical mean =\", np.mean(light_speed))\n",
    "print(\"Empirical std =\", np.std(light_speed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1cce77-6b7d-4e74-ade1-6c4b0fcdb613",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise**. Determine the 95% credibility interval for $\\mu$ and $\\sigma$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c800107",
   "metadata": {},
   "source": [
    "A (highest posterior density) 95% credibility interval is the shortest interval that contains 95% of the posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b5cdb-41d4-43af-bbf6-f812993a7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpd(trace, mass_frac) :\n",
    "    d = np.sort(np.copy(trace))\n",
    "    n = len(trace)\n",
    "    n_samples = np.floor(mass_frac * n).astype(int)\n",
    "    int_width = d[n_samples:] - d[:n-n_samples]\n",
    "    min_int = np.argmin(int_width)\n",
    "    return np.array([d[min_int], d[min_int+n_samples]])\n",
    "\n",
    "low, high = hpd(thetas[:, 0], 0.95)\n",
    "low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "24800+low, 24800+high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4bb01",
   "metadata": {},
   "source": [
    "Note: The currently accepted estimate of the speed of light under the conditions of the experiment corresponds to a measurement of 24833.02 nanoseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34aaa5-957c-4ed2-9ad0-881f01a158fc",
   "metadata": {},
   "source": [
    "# Posterior predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f62b5-c5fd-4cf1-9805-a9a59130c08f",
   "metadata": {},
   "source": [
    "$$p(x^\\text{rep} | \\{x\\}) = \\int p(x^\\text{rep} | \\theta) p(\\theta | \\{x\\}) d\\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c139ad-bbcf-4090-9c31-86bdeaf6ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = thetas[:1000]\n",
    "\n",
    "def pp(thetas):\n",
    "    return norm.rvs(loc=thetas[:, 0], scale=thetas[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab0fefc-f291-48fb-98fd-d09da8711046",
   "metadata": {},
   "source": [
    "$$p(\\{x^\\text{rep}\\} | \\{x\\}) = \\int p(\\{x^\\text{rep}\\} | \\theta) p(\\theta | \\{x\\}) d\\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcad386-1338-42e0-b07c-b76480393bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate(n_samples, thetas):\n",
    "    x = np.zeros((n_samples, len(thetas)))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        x[i] = pp(thetas)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57106a33-2298-45ee-bce3-59f31be5cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicas = replicate(66, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bacca-bd90-4103-b9a5-a10010d9e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd1d7a-0f4f-4d1b-ab06-b83900c7513a",
   "metadata": {},
   "source": [
    "## Visual inspection of posterior predictives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ff510-397b-4d93-b9d7-c06fb56bebcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.hist(light_speed, bins=15, range=(-50, 50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac94d5df-9d92-4a6e-ab19-585e0892e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, sharex=True, sharey=True)\n",
    "lower = -50\n",
    "upper = np.max(replicas[:, :25])\n",
    "for k in range(25):\n",
    "    ax[k // 5, k % 5].hist(replicas[:, k], bins=15, range=(lower, upper))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3050589-afc9-4c6a-8ca1-a663b0651f8d",
   "metadata": {},
   "source": [
    "## Test quantities\n",
    "\n",
    "Statistics 101: _There is only one test!_\n",
    "1. Given the data, compute a test statistic that measures some aspect of the data.\n",
    "2. Compute the distribution of the test statistic under the model.\n",
    "3. Compare the observed test statistic to the distribution of the test statistic under the model, usually in terms of a p-value.\n",
    "\n",
    "Forget about statistical recipes! They have been invented when computers were not available, i.e. when Step 2 was not something we could simply do by simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a38ee-d7f3-4637-a6ac-8d5b689d0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_speed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b5701-f5bd-4f0a-84c5-1d3ce38bfcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.hist(replicas.mean(axis=0), bins=15)\n",
    "ax.axvline(light_speed.mean(), color=\"r\")\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1da69c-520c-4b59-8996-6c41fe7e77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.hist(replicas.std(axis=0), bins=15)\n",
    "ax.axvline(light_speed.std(), color=\"r\")\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d83bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.hist(replicas.min(axis=0), bins=15)\n",
    "ax.axvline(light_speed.min(), color=\"r\")\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec520ce-5ce0-489c-9378-9e2e9165c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(replicas.min(axis=0) >= light_speed.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a432837",
   "metadata": {},
   "source": [
    "This suggests that the normal distribution is not adequate to model the data. Its tails are too light."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d98364-55fd-4c75-92fc-171306dc10ea",
   "metadata": {},
   "source": [
    "## Bayesian p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19edab37-051d-4670-9c41-d4464b21da4d",
   "metadata": {},
   "source": [
    "Formally,\n",
    "- $T(\\{x\\}, \\theta)$ denotes a test quantity or discrepancy measure.\n",
    "- $T(\\{x\\})$ denotes a test-statistic.\n",
    "\n",
    "Classical p-values:\n",
    "$$p_C = P(T(\\{x^\\text{rep}\\}) \\geq T(\\{x\\}) | \\theta)$$\n",
    "\n",
    "Bayesian p-values:\n",
    "$$p_B = P(T(\\{x^\\text{rep}\\}, \\theta) \\geq T(\\{x\\}, \\theta) | \\{ x \\})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776f22e-1261-4057-ba4e-9923afbc5617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(xs, theta):\n",
    "    xs = np.sort(xs)\n",
    "    return np.abs(xs[60] - theta) - np.abs(xs[5] - theta)  # should scatter around 0 for a symmetric distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1318b9-be36-477a-951f-bdbbdb2fdaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_reps = []\n",
    "T_x = []\n",
    "\n",
    "for k in range(len(thetas)):\n",
    "    T_reps.append(T(replicas[:, k], thetas[k, 0]))\n",
    "    T_x.append(T(light_speed, thetas[k, 0]))\n",
    "    \n",
    "T_reps = np.array(T_reps)\n",
    "T_x = np.array(T_x)\n",
    "\n",
    "# T_reps is the test statistic for the replicas\n",
    "# T_x is the test statistic for the original data \n",
    "# both are computed for the same value of theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d342694-37b6-4a4e-a929-5d1a570a7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter(T_x, T_reps)\n",
    "ax.plot([-10, 12], [-10, 12])\n",
    "ax.set(xlim=(-10, 12), ylim=(-10,12), xlabel=r\"$T(\\{x\\}, \\theta)$\", ylabel=r\"$T(\\{x^{rep}\\}, \\theta)$\")\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcca85-714a-4442-abf3-9726d00ce47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(np.hstack([T_x.reshape(-1, 1), T_reps.reshape(-1, 1)]), labels=[r\"$T(\\{x\\}, \\theta)$\", r\"$T(\\{x^{rep}\\}, \\theta)$\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e0e17-c68d-40b0-9b04-4f5f5babeb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(T_reps >= T_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af31f9e-5c51-421a-abfe-375628f06a55",
   "metadata": {},
   "source": [
    "_Gelman:_ \"Finding an extreme p-value and thus ‘rejecting’ a model is\n",
    "never the end of an analysis; the departures of the test quantity in\n",
    "question from its posterior predictive distribution will often suggest\n",
    "improvements of the model or places to check the data, as in the speed\n",
    "of light example. Moreover, even when the current model seems\n",
    "appropriate for drawing inferences (in that no unusual deviations\n",
    "between the model and the data are found), the next scientific step will\n",
    "often be a more rigorous experiment incorporating additional factors,\n",
    "thereby providing better data.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dats0001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
