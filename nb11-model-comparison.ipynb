{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05080d5f-640d-4a95-bba4-f0dadda33ce5",
   "metadata": {},
   "source": [
    "# `nb11`: Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72dc22d-a680-4c48-aebe-20cd35508c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec084d85-4568-4c66-8688-8a08f8823560",
   "metadata": {},
   "source": [
    "# Model comparison using Bayes factors\n",
    "\n",
    "## Marginal likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028f340-2850-438a-84f3-f281522a9dd5",
   "metadata": {},
   "source": [
    "$$p(\\theta | x, \\mathcal{M}_1) = \\frac{p(x | \\theta, \\mathcal{M}_1) p(\\theta | \\mathcal{M}_1)}{p(x | \\mathcal{M}_1)}$$\n",
    "\n",
    "$$p(x | \\mathcal{M}_1) = \\int p(x | \\theta, \\mathcal{M}_1) p(\\theta | \\mathcal{M}_1) d\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3357a0-b777-4db9-b54b-131bfede3e18",
   "metadata": {},
   "source": [
    "We assume a study where we assess the number of successes observed in a fixed number of trials. For example, we have 80 successes out of 100 trials. \n",
    "\n",
    "A simple model of this data can be built by assuming that the data is distributed according to a binomial distribution. In a binomial distribution, $n$ independent experiments are performed and the result of each experiment is succesful with probability $\\theta$. Then, the binomial distribution is the probability distribution of the number of successes $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fef37-c596-4886-b3bb-9ae7f6b61120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "from scipy.stats import binom\n",
    "\n",
    "def likelihood1(theta, k, n):\n",
    "    return binom.pmf(k, n, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ddb90-9c1c-46fd-96e9-7c939b77a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "theta = 0.6\n",
    "n = 100\n",
    "\n",
    "ks = range(0, n+1)\n",
    "ax.plot(ks, likelihood1(theta, ks, n))\n",
    "ax.set(xlabel=r\"$k$\", ylabel=r\"$p(k | n={}, \\theta={})$\".format(n, theta))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e038d-a425-4a92-87f5-70855f8dff0f",
   "metadata": {},
   "source": [
    "Suppose we have prior information about the probability parameter $\\theta$, encoded as a Beta distribution of parameters $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d04c84-fcaf-4edf-ae3d-f1ba104093be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "a = 4\n",
    "b = 2\n",
    "\n",
    "def prior1(theta):\n",
    "    return beta.pdf(theta, a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667df1c-db68-4d60-af33-a4d9c8ec952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "thetas = np.linspace(0, 1, num=100)\n",
    "ax.plot(thetas, prior1(thetas))\n",
    "ax.set(xlabel=r\"$\\theta$\", ylabel=r\"$p(\\theta)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1e8bf-89b2-4878-a4c6-37c5f24ee054",
   "metadata": {},
   "source": [
    "Then the marginal likelihood is then given by the integral of the product of the likelihood and the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395cd7b-2c40-466e-a918-e6c217ccc5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "ml1, _ = quad(lambda theta: likelihood1(theta, k=80, n=n) * prior1(theta), 0, 1)\n",
    "ml1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170d37b-10cb-422c-8f98-d0ddec099879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: wider prior\n",
    "def prior2(theta):\n",
    "    return beta.pdf(theta, a=1, b=1)\n",
    "\n",
    "ml2, _ = quad(lambda theta: likelihood1(theta, k=80, n=n) * prior2(theta), 0, 1)\n",
    "ml2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d32917-e629-4e81-b644-53b84b95448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: more complex likelihood\n",
    "from scipy.stats import betabinom, lognorm\n",
    "\n",
    "def likelihood3(alpha, beta, k, n):\n",
    "    return betabinom.pmf(k, n, alpha, beta)\n",
    "\n",
    "def prior3(alpha, beta):\n",
    "    return lognorm.pdf(alpha, 100) * lognorm.pdf(beta, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582cd84-df61-40a3-871c-558249d40087",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "vs = np.linspace(0, 100)\n",
    "ax.plot(vs, lognorm.pdf(vs, 100))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c6588-4d31-4795-8f4a-1e3403864287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import dblquad\n",
    "ml3, _ = dblquad(lambda alpha, beta: likelihood3(alpha, beta, k=80, n=n) * prior3(alpha, beta), 0, np.inf, 0, np.inf)\n",
    "ml3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b049dc-78c5-478c-a8f4-ab548f68a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model 1:\", ml1)\n",
    "print(\"Model 2:\", ml2)\n",
    "print(\"Model 3:\", ml3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d005ffc-8c56-4ce2-96a8-7cb615fe114f",
   "metadata": {},
   "source": [
    "## Bayes factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e8f4b-23dc-4b34-b1a4-5aea50ae74c2",
   "metadata": {},
   "source": [
    "$$\\text{BF}_{1,2} = \\frac{p(x | \\mathcal{M}_1)}{p(x | \\mathcal{M}_2)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab30c5-3e39-4a1b-9819-9e2f09472c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BF(1,2):\", ml1 / ml2, np.log(ml1 / ml2))\n",
    "print(\"BF(1,3):\", ml1 / ml3, np.log(ml1 / ml3))\n",
    "print(\"BF(2,3):\", ml2 / ml3, np.log(ml2 / ml3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571a89f-a9d8-4d85-a512-1983f9ad7a16",
   "metadata": {},
   "source": [
    "If we want to know, given the data $x$, how much more probable model $\\mathcal{M}_1$ is than model $\\mathcal{M}_2$, then we need the prior odds:\n",
    "\n",
    "$$\\frac{p(\\mathcal{M}_1 | x)}{p(\\mathcal{M}_2 | x)} = \\frac{p(\\mathcal{M}_1)}{p(\\mathcal{M}_2)} \\frac{p(x | \\mathcal{M}_1)}{p(x | \\mathcal{M}_2)} = \\frac{p(\\mathcal{M}_1)}{p(\\mathcal{M}_2)} \\text{BF}_{1,2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d20df7-e069-4209-bf54-8fc080e08e91",
   "metadata": {},
   "source": [
    "![](figures/nb11/bf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fed4c-f739-46e0-ae18-77a5c8104857",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise**. Using the light speed experiment data from `nb10`, compare:\n",
    "- $\\mathcal{M}_1: p(x | \\mu, \\sigma) = \\mathcal{N}(x | \\mu, \\sigma), p(\\mu, \\sigma) = \\mathcal{U}$\n",
    "- $\\mathcal{M}_2: p(x | \\mu, \\sigma, a) = \\text{skewnorm}(x | \\mu, \\sigma, a), p(\\mu, \\sigma, a) = \\mathcal{U}$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126d5ad-a9d0-4c2d-a05a-7dc3cb72180f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff3f4fa7-c60d-4b9b-ac62-a162f396f606",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise**. How to compute the Bayes factor for more complicated models?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890697c-8a53-404c-95c2-7faeb2d9a3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfed2a7d-bc08-4fc5-aeab-4e9c48ae1b4a",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027fedb-3bcc-4f89-8232-fcbad320e398",
   "metadata": {},
   "source": [
    "A popular way to evaluate and compare models is on their ability to make predictions for future or unseen observations $x_\\text{pred}$, using what we learned from the observed data $\\{x\\} = \\{x_1, ..., x_n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237e60a-f71c-4889-ba3d-8d078c579977",
   "metadata": {},
   "source": [
    "## Expected log predictive density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e8a8c-4099-491f-b96b-53d6f10e5931",
   "metadata": {},
   "source": [
    "Let $$u(\\mathcal{M}_1, x_\\text{pred}) = \\log p(x_\\text{pred} | \\{x\\}, \\mathcal{M}_1)$$\n",
    "\n",
    "$$\\log p(x_\\text{pred} | \\{x\\}, \\mathcal{M}_1) = \\log \\int p(x_\\text{pred} | \\theta, \\mathcal{M}_1) p(\\theta | \\{x\\}, \\mathcal{M}_1) d\\theta $$\n",
    "\n",
    "$$\\text{elpd} = u(\\mathcal{M}_1) = \\mathbb{E}_{p_\\text{true}(x_\\text{pred})} [ \\log p(x_\\text{pred} | \\{x\\}, \\mathcal{M}_1) ]$$\n",
    "\n",
    "where $p_\\text{true}$ is the true data generating distribution. \n",
    "\n",
    "The model with the highest $\\text{elpd}$ is the the model with the predictions that are the closest to the ones of the true data generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb90917-766b-4244-8ca2-eb102facd4e8",
   "metadata": {},
   "source": [
    "But we we don't know $p_\\text{true}$! Solution: use the $n$ observations from $\\{x\\}$:\n",
    "\n",
    "$$\\widehat{\\text{elpd}} = \\frac{1}{n} \\sum_{i=1}^n \\log p(x_i | \\{x\\}, \\mathcal{M}_1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348864b-0913-400d-97a1-06cf71d467ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "theta_true = np.array([1.0, 3.0])\n",
    "p_true = beta(a=theta_true[0], b=theta_true[1])\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "xs = p_true.rvs(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae78204-705f-40f9-acbe-127d88bf1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "vs = np.linspace(0, 1)\n",
    "ax.plot(vs, p_true.pdf(vs))\n",
    "ax.hist(xs, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18744c51-c17c-4414-b5f3-bb2b0626e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "def log_likelihood(theta, x):\n",
    "    return beta(theta[0], theta[1]).logpdf(x).sum()\n",
    "\n",
    "def log_prior(theta):\n",
    "    return lognorm.pdf(theta[0], 10) * lognorm.pdf(theta[1], 10)\n",
    "\n",
    "def log_posterior(theta, x):\n",
    "    if theta[0] <= 0 or theta[1] <= 0:\n",
    "        return -np.inf\n",
    "    else:                          \n",
    "        return log_likelihood(theta, x) + log_prior(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae458e-198a-464b-af6a-6d431d9d4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the posterior\n",
    "import emcee\n",
    "pos = np.array([1.0, 1.0]) + 0.1 * np.random.randn(5, 2)\n",
    "nwalkers, ndim = pos.shape\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(xs,))\n",
    "sampler.run_mcmc(pos, 1000, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6883bc-4f0d-4e46-bf8b-a6d9aa67ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "thetas = sampler.get_chain(flat=True, discard=100)\n",
    "print(len(thetas))\n",
    "fig = corner.corner(thetas, labels=[\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861476c5-aa46-4e23-a561-555da2d7c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elpd\n",
    "def log_posterior_predictive(x_i, thetas):\n",
    "    return np.mean([log_likelihood(theta, x_i) for theta in thetas])\n",
    "\n",
    "def elpd(xs, thetas):\n",
    "    return np.mean([log_posterior_predictive(x_i, thetas) for x_i in xs])\n",
    "\n",
    "subset = np.random.permutation(len(thetas))[:500]\n",
    "elpd(xs, thetas[subset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d10b96-1397-4bc5-8030-336880937d95",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise**. What's wrong with this procedure?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bf9fc-f9f3-4f08-bd5b-2dc6a49c32e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cafde425-e4cc-411b-a8f6-0833e1e11eea",
   "metadata": {},
   "source": [
    "## K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12efe9-bb13-411f-8b33-6eb61633fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "elpd_test = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(xs):\n",
    "    # posterior fit\n",
    "    pos = np.array([1.0, 1.0]) + 0.1 * np.random.randn(5, 2)\n",
    "    nwalkers, ndim = pos.shape\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(xs[train_idx],))\n",
    "    sampler.run_mcmc(pos, 1000, progress=True);\n",
    "    thetas = sampler.get_chain(flat=True, discard=100)\n",
    "    \n",
    "    # epld\n",
    "    subset = np.random.permutation(len(thetas))[:500]\n",
    "    elpd_test.append(elpd(xs[test_idx], thetas[subset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32f432-fcac-43ef-af81-7214ccfca624",
   "metadata": {},
   "outputs": [],
   "source": [
    "elpd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5626a279-8ca8-4327-8e02-8d7203a1dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(elpd_test), np.std(elpd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2800ca6e-88b5-4806-b51e-6733c679b39b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise**. Evaluate the K-Fold estimate as a function of $K$. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93777fd5-6c3b-4c24-b778-4207d6642543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dats0001] *",
   "language": "python",
   "name": "conda-env-dats0001-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
